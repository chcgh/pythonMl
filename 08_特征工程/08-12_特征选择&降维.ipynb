{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold,SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [0, 2, 0, 3],\n",
    "    [0, 1, 4, 3],\n",
    "    [0.1, 1, 1, 3],\n",
    "    [1, 2, 3, 1],\n",
    "    [2, 3, 4, 3]\n",
    "], dtype=np.float32)\n",
    "Y = np.array([1,2,1,2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方差选择法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于方差选择最优的特征属性\n",
    "#方差小于阈值的列被删除\n",
    "variance = VarianceThreshold(threshold=0.6)\n",
    "variance.fit(X)\n",
    "print(\"各个特征属性的方差为:\")\n",
    "print(variance.variances_)\n",
    "print('-----------------')\n",
    "print(variance.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SelectKBest\n",
    "SelectKBest是scikit-learn库中的一个特征选择函数，用于从数据集中选择k个最佳特征。它可以根据给定的评价函数和得分，来选择和排名特征。\n",
    "\n",
    "SelectKBest可以用于以下两种情况：\n",
    "\n",
    "- 想要减少数据集的维度，仅使用最重要的特征。\n",
    "\n",
    "- 想要获得每个特征的重要程度排名，以便进一步分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相关系数法\n",
    "\n",
    "- F值：指特征和目标变量之间的关系是否显著，是一个统计指标。F值越大，表示特征和目标变量之间的相关性越强。\n",
    "- 相关系数：指特征和目标变量之间的线性关系的强度和方向，可以是正相关、负相关或无关。相关系数的取值范围为[-1,1]，绝对值越大，表示相关性越强，符号表示相关性的方向。\n",
    "\n",
    "F值和相关系数是两个不同的指标，F值反映特征和目标变量之间的关系是否显著，相关系数反映特征和目标变量之间的线性关系的强度和方向。在特征选择中，通常使用F值作为评价函数，选择与目标变量相关性较强的特征，或者使用相关系数作为评价指标，评价每个特征与目标变量之间的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#相关系数最大的k列被保留\n",
    "sk1 = SelectKBest(f_regression, k=2) #评分函数是 f_regression，这个函数使用 F 检验来计算特征的相关性。\n",
    "sk1.fit(X,Y)\n",
    "print(sk1)\n",
    "print('------------')\n",
    "print(sk1.scores_)\n",
    "print('------------')\n",
    "print(sk1.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卡方检验\n",
    "\n",
    "卡方检验用于度量特征和目标变量之间的关联性，可以应用于离散特征的选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用chi2的时候要求特征属性的取值为非负数\n",
    "sk2 = SelectKBest(chi2, k=2)\n",
    "sk2.fit(X, Y)\n",
    "print(sk2)\n",
    "print(sk2.scores_)\n",
    "print(sk2.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "X = np.array([\n",
    "    [-1, -1, 3, 1], \n",
    "    [-2, -1, 2, 4], \n",
    "    [-3, -2, 4, 5], \n",
    "    [1, 1, 5, 4], \n",
    "    [2, 1, 6, -5], \n",
    "    [3, 2, 1, 5]])\n",
    "y = np.array([1, 1, 2, 2, 0, 1])\n",
    "# n_components：给定降低到多少维度，不能大于min(n_features,n_class-1)\n",
    "clf = LinearDiscriminantAnalysis(n_components=2)\n",
    "clf.fit(X, y)\n",
    "print(clf.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## PCA降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139.81349365  49.92833765   0.14650204   0.        ]\n",
      "[0.73629323 0.26293526 0.00077152 0.        ]\n",
      "[[-10.11606313   6.492326     0.31419724   0.        ]\n",
      " [ 10.80754053   5.73455069  -0.33278523   0.        ]\n",
      " [-10.34733219  -6.08709685  -0.32972476  -0.        ]\n",
      " [  9.65585479  -6.13977984   0.34831276  -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X2 = np.array([\n",
    "    [ 5.1,  3.5,  1.4,  0.2, 1, 23],\n",
    "    [ 4.9,  3. ,  1.4,  0.2, 2.3, 2.1],\n",
    "    [ -6.2,  0.4,  5.4,  2.3, 2, 23],\n",
    "    [ -5.9,  0. ,  5.1,  1.8, 2, 3]\n",
    "], dtype=np.float64)\n",
    "# n_components: 给定降低到多少维度\n",
    "#               小于1时，表示保留的维度的重要性的占比，比如0.9表示占比为90%\n",
    "#               大于等于1时，表示保留的维度，但是该值必须小于等于min(样本数目,特征数目)\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(X2)\n",
    "# print(pca.mean_)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "# print(pca.components_)\n",
    "print(pca.transform(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=112\n",
    "print(np.power(1.01,1/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.var(np.vstack([[[65, 174]], [[60, 170]]]), axis=0, ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
